# Voxio Bot Configuration (voxio.bot)

[identity]
name = "Vinston Wolf"
greeting = "Vinston here. What needs fixing?"

[stt]
# Provider: openai, mlx-whisper, whisper
provider = "mlx-whisper"
# Models: tiny, base, small, medium, large, large-turbo, large-turbo-q4, distil-large
model = "large-turbo"

[tts]
# Provider: elevenlabs, piper
provider = "elevenlabs"
# Voice ID (for ElevenLabs)
voice = "QzTKubutNn9TjrB7Xb2Q"

[handoff]
# Type: none, prompt, ambient
# - none: no sound while waiting
# - prompt: generate via ElevenLabs SFX API
# - ambient: loop random files until response/interruption
type = "ambient"

# Prompt for type = "prompt" (ElevenLabs sound generation)
prompt = "soft keyboard typing sounds"

# File list for type = "ambient"
files = [
    "sounds/handoff_typing.wav",
    "sounds/110451__freeborn__paper01.wav",
    "sounds/377260__johnnypanic__clearing-throat-2.wav",
    "sounds/414819__bokal__office-drawer.wav",
]

# Gap between ambient sounds (seconds)
gap = 0.5

[video]
input_enabled = true
output_enabled = true
avatar = "avatar.png"
# How long to display generated images before returning to avatar (seconds)
image_display_duration = 300

[vision]
# Vision analysis mode: handoff, direct, or local
# - handoff: send to Clawdbot for analysis (default)
# - direct: feed image directly to voice LLM (Claude vision)
# - local: use a local vision model
mode = "handoff"

# Local vision model (only used when mode = "local")
# Options: moondream, llava, florence-2
local_model = "moondream"
# Path to local model (if required)
# local_model_path = "/path/to/model"

[vad]
# Silence duration before turn is considered complete (seconds)
stop_secs = 0.3

[transport]
# Transport type: webrtc (local client) or daily (Daily.co/playground)
type = "webrtc"
# Port for local WebRTC server (webrtc transport only)
port = 8086
# Daily.co room URL (daily transport only, or set DAILY_ROOM_URL env)
# daily_room_url = "https://your-domain.daily.co/room-name"
